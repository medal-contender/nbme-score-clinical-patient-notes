{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n# This must be done before importing transformers\nimport shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    \n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-12T10:28:20.684306Z","iopub.execute_input":"2022-03-12T10:28:20.68504Z","iopub.status.idle":"2022-03-12T10:28:20.698485Z","shell.execute_reply.started":"2022-03-12T10:28:20.684997Z","shell.execute_reply":"2022-03-12T10:28:20.697824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport ast\nimport sys\nimport copy\nimport json\nimport math\nimport string\nimport pickle\nimport random\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:20.91517Z","iopub.execute_input":"2022-03-12T10:28:20.915413Z","iopub.status.idle":"2022-03-12T10:28:23.435441Z","shell.execute_reply.started":"2022-03-12T10:28:20.915386Z","shell.execute_reply":"2022-03-12T10:28:23.434659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=[ \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold0_best.pth\",\n          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold1_best.pth\",\n          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold2_best.pth\",\n          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold3_best.pth\",\n          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold4_best.pth\"\n         ]\n    config_path='../input/nbme-debertav2-10fold/config.pth'\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:23.437386Z","iopub.execute_input":"2022-03-12T10:28:23.437916Z","iopub.status.idle":"2022-03-12T10:28:23.443856Z","shell.execute_reply.started":"2022-03-12T10:28:23.437875Z","shell.execute_reply":"2022-03-12T10:28:23.443007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/deberta-deepshare/\"\n#     config_path= '../input/deberta-v3-large-5-folds-public/config.pth'\n#     model=\"microsoft/deberta-v3-large\"\n#     batch_size=32\n#     fc_dropout=0.2\n#     max_len=354\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:23.44535Z","iopub.execute_input":"2022-03-12T10:28:23.446011Z","iopub.status.idle":"2022-03-12T10:28:23.452896Z","shell.execute_reply.started":"2022-03-12T10:28:23.445975Z","shell.execute_reply":"2022-03-12T10:28:23.452247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:23.454573Z","iopub.execute_input":"2022-03-12T10:28:23.456611Z","iopub.status.idle":"2022-03-12T10:28:23.468343Z","shell.execute_reply.started":"2022-03-12T10:28:23.456571Z","shell.execute_reply":"2022-03-12T10:28:23.467614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:23.469796Z","iopub.execute_input":"2022-03-12T10:28:23.470212Z","iopub.status.idle":"2022-03-12T10:28:24.712949Z","shell.execute_reply.started":"2022-03-12T10:28:23.470177Z","shell.execute_reply":"2022-03-12T10:28:24.711969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Helper functions for scoring","metadata":{}},{"cell_type":"code","source":"\n# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    \n    return f1_score(truths, preds)\n\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n        \n    return binary\n\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n        \n    return micro_f1(bin_preds, bin_truths)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:24.717896Z","iopub.execute_input":"2022-03-12T10:28:24.718153Z","iopub.status.idle":"2022-03-12T10:28:24.734903Z","shell.execute_reply.started":"2022-03-12T10:28:24.718119Z","shell.execute_reply":"2022-03-12T10:28:24.734081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n        \n    return truths\n\n\ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n            \n    return results\n\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n        \n    return results\n\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n        \n    return predictions\n\n\ndef get_score(y_true, y_pred):\n    return span_micro_f1(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:24.736334Z","iopub.execute_input":"2022-03-12T10:28:24.736711Z","iopub.status.idle":"2022-03-12T10:28:24.75995Z","shell.execute_reply.started":"2022-03-12T10:28:24.736676Z","shell.execute_reply":"2022-03-12T10:28:24.758941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# OOF","metadata":{}},{"cell_type":"code","source":"# oof1 = pd.read_pickle(\"../input/nbmedebertadeepsharefclayer/oof_df01.pkl\")\n# oof2 = pd.read_pickle(\"../input/nbmedebertadeepsharefclayer/oof_df23.pkl\")\n# oof3 = pd.read_pickle(\"../input/nbmedebertadeepsharefclayer/oof_df4.pkl\")\n# oof_new_deepshare = pd.concat([oof1, oof2, oof3]).reset_index()\n\n# oof = oof_new_deepshare\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.20, 0.70, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n# LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:24.964076Z","iopub.execute_input":"2022-03-12T10:28:24.964302Z","iopub.status.idle":"2022-03-12T10:28:24.968267Z","shell.execute_reply.started":"2022-03-12T10:28:24.964275Z","shell.execute_reply":"2022-03-12T10:28:24.967641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof = pd.read_pickle('../input/nbme-debertav2-10fold/oof_df.pkl')\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.40, 0.60, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n# LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:25.190946Z","iopub.execute_input":"2022-03-12T10:28:25.191197Z","iopub.status.idle":"2022-03-12T10:28:25.195641Z","shell.execute_reply.started":"2022-03-12T10:28:25.191168Z","shell.execute_reply":"2022-03-12T10:28:25.19479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof_883 = pd.read_pickle('../input/deberta-v3-large-5-folds-public/oof_df.pkl')\n\n# oof_888 = pd.read_pickle('../input/nbme-debertav2-10fold/oof_df.pkl')\n\n# oof4 = pd.read_pickle(\"../input/debertafclayerinite/oof_df01.pkl\")\n# oof5 = pd.read_pickle(\"../input/debertafclayerinite/oof_df23.pkl\")\n# oof6 = pd.read_pickle(\"../input/debertafclayerinite/oof_df4.pkl\")\n# oof_882 = pd.concat([oof4, oof5, oof6]).reset_index()\n\n# oof7 = pd.read_pickle(\"../input/deberta-deepshare/oof_df01.pkl\")\n# oof8 = pd.read_pickle(\"../input/deberta-deepshare/oof_df23.pkl\")\n# oof9 = pd.read_pickle(\"../input/deberta-deepshare/oof_df4.pkl\")\n# oof_882_deep = pd.concat([oof7, oof8, oof9]).reset_index()\n\n# oof = oof_882_deep\n# for i in range(354):\n#     oof[i] = ( oof_883[i] + oof_882_deep[i] ) / 2\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.38, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     print(f\"th: {th}  score: {score:.5f}\")\n# print(f\"best_th: {best_th}  score: {best_score:.5f}\")\n\n# print(\"=======================\")\n\n# oof = oof_882_deep\n# for i in range(354):\n#     oof[i] = ( oof_883[i] + oof_882_deep[i] ) / 2\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.38, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     print(f\"th: {th}  score: {score:.5f}\")\n# print(f\"best_th: {best_th}  score: {best_score:.5f}\")\n\n# print(\"=======================\")\n\n# oof = oof_882_deep\n# for i in range(354):\n#     oof[i] = ( oof_882[i] + oof_882_deep[i] ) / 2\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.38, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     print(f\"th: {th}  score: {score:.5f}\")\n# print(f\"best_th: {best_th}  score: {best_score:.5f}\")\n\n# print(\"=======================\")\n\n# oof = oof_882_deep\n# for i in range(354):\n#     oof[i] = ( oof_883[i] + oof_882[i] + oof_882_deep[i] ) / 3\n\n# truths = create_labels_for_scoring(oof)\n# char_probs = get_char_probs(oof['pn_history'].values,\n#                             oof[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.38, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     print(f\"th: {th}  score: {score:.5f}\")\n# print(f\"best_th: {best_th}  score: {best_score:.5f}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:25.471218Z","iopub.execute_input":"2022-03-12T10:28:25.471494Z","iopub.status.idle":"2022-03-12T10:28:25.478126Z","shell.execute_reply.started":"2022-03-12T10:28:25.471462Z","shell.execute_reply":"2022-03-12T10:28:25.477134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# oof34234 = oof_new_deepshare[:]\n# for i in range(354):\n#     oof34234[i] = ( oof_883[i] + oof_new_deepshare[i] ) / 2 # oof_new_deepshare oof_882_deep\n\n# truths = create_labels_for_scoring(oof444)\n# char_probs = get_char_probs(oof34234['pn_history'].values,\n#                             oof34234[[i for i in range(CFG.max_len)]].values, \n#                             CFG.tokenizer)\n# best_th = 0.5\n# best_score = 0.\n# for th in np.arange(0.38, 0.55, 0.01):\n#     th = np.round(th, 2)\n#     results = get_results(char_probs, th=th)\n#     preds = get_predictions(results)\n#     score = get_score(preds, truths)\n\n#     if best_score < score:\n#         best_th = th\n#         best_score = score\n#     print(f\"th: {th}  score: {score:.5f}\")\n# print(f\"best_th: {best_th}  score: {best_score:.5f}\")\n\n# 883 + new_deepshare : best_th: 0.47  score: 0.88561\n# 882 + new_deepshare : best_th: 0.5  score: 0.88499\n# deepshare + new_deepshare : best_th: 0.55  score: 0.88498\n# 883 + 882 + new_deepshare : best_th: 0.49  score: 0.88534\n# 883 + deepshare : best_th: 0.48  score: 0.88397\n# 882 + deepshare : best_th: 0.47  score: 0.88471","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:25.836471Z","iopub.execute_input":"2022-03-12T10:28:25.836721Z","iopub.status.idle":"2022-03-12T10:28:25.840988Z","shell.execute_reply.started":"2022-03-12T10:28:25.836692Z","shell.execute_reply":"2022-03-12T10:28:25.840272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Loading","metadata":{}},{"cell_type":"code","source":"def preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\n\n\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\nfeatures = preprocess_features(features)\n\nprint(f\"test.shape: {test.shape}\")\ndisplay(test.head())\nprint(f\"features.shape: {features.shape}\")\ndisplay(features.head())\nprint(f\"patient_notes.shape: {patient_notes.shape}\")\ndisplay(patient_notes.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:26.628149Z","iopub.execute_input":"2022-03-12T10:28:26.628404Z","iopub.status.idle":"2022-03-12T10:28:27.41844Z","shell.execute_reply.started":"2022-03-12T10:28:26.628375Z","shell.execute_reply":"2022-03-12T10:28:27.417739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\ndisplay(test.head())","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:27.420234Z","iopub.execute_input":"2022-03-12T10:28:27.420777Z","iopub.status.idle":"2022-03-12T10:28:27.453185Z","shell.execute_reply.started":"2022-03-12T10:28:27.420729Z","shell.execute_reply":"2022-03-12T10:28:27.45248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"code","source":"def prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n        \n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        \n        return inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:29.800318Z","iopub.execute_input":"2022-03-12T10:28:29.800919Z","iopub.status.idle":"2022-03-12T10:28:29.808159Z","shell.execute_reply.started":"2022-03-12T10:28:29.800879Z","shell.execute_reply":"2022-03-12T10:28:29.807477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"# deepshare model\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout_0 = nn.Dropout(0.1)\n        self.fc_dropout_1 = nn.Dropout(0.2)\n        self.fc_dropout_2 = nn.Dropout(0.3)\n        self.fc_dropout_3 = nn.Dropout(0.4)\n        self.fc_dropout_4 = nn.Dropout(0.5)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output_0 = self.fc(self.fc_dropout_0(feature))\n        output_1 = self.fc(self.fc_dropout_1(feature))\n        output_2 = self.fc(self.fc_dropout_2(feature))\n        output_3 = self.fc(self.fc_dropout_3(feature))\n        output_4 = self.fc(self.fc_dropout_4(feature))\n        output = (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:33.83387Z","iopub.execute_input":"2022-03-12T10:28:33.834576Z","iopub.status.idle":"2022-03-12T10:28:33.849345Z","shell.execute_reply.started":"2022-03-12T10:28:33.834537Z","shell.execute_reply":"2022-03-12T10:28:33.846757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # deepshare model\n# class CustomModel(nn.Module):\n#     def __init__(self, cfg, config_path=None, pretrained=False):\n#         super().__init__()\n#         self.cfg = cfg\n#         if config_path is None:\n#             self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n#         else:\n#             self.config = torch.load(config_path)\n#         if pretrained:\n#             self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n#         else:\n#             self.model = AutoModel.from_config(self.config)\n#         self.fc_dropout_0 = nn.Dropout(0.1)\n#         self.fc_dropout_1 = nn.Dropout(0.2)\n#         self.fc_dropout_2 = nn.Dropout(0.3)\n#         self.fc_dropout_3 = nn.Dropout(0.4)\n#         self.fc_dropout_4 = nn.Dropout(0.5)\n#         self.fc = nn.Linear(self.config.hidden_size, 1)\n#         self._init_weights(self.fc)\n        \n#     def _init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.bias is not None:\n#                 module.bias.data.zero_()\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n#         elif isinstance(module, nn.LayerNorm):\n#             module.bias.data.zero_()\n#             module.weight.data.fill_(1.0)\n        \n#     def feature(self, inputs):\n#         outputs = self.model(**inputs)\n#         last_hidden_states = outputs[0]\n#         return last_hidden_states\n\n#     def forward(self, inputs):\n#         feature = self.feature(inputs)\n#         output_0 = self.fc(self.fc_dropout_0(feature))\n#         output_1 = self.fc(self.fc_dropout_1(feature))\n#         output_2 = self.fc(self.fc_dropout_2(feature))\n#         output_3 = self.fc(self.fc_dropout_3(feature))\n#         output_4 = self.fc(self.fc_dropout_4(feature))\n#         output = (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:34.17303Z","iopub.execute_input":"2022-03-12T10:28:34.173814Z","iopub.status.idle":"2022-03-12T10:28:34.179588Z","shell.execute_reply.started":"2022-03-12T10:28:34.17376Z","shell.execute_reply":"2022-03-12T10:28:34.178784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"def inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    \n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    \n    return predictions","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:34.985556Z","iopub.execute_input":"2022-03-12T10:28:34.985819Z","iopub.status.idle":"2022-03-12T10:28:34.991757Z","shell.execute_reply.started":"2022-03-12T10:28:34.98579Z","shell.execute_reply":"2022-03-12T10:28:34.99102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor path in CFG.path:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(path,\n                           map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:28:35.24438Z","iopub.execute_input":"2022-03-12T10:28:35.245067Z","iopub.status.idle":"2022-03-12T10:31:27.934811Z","shell.execute_reply.started":"2022-03-12T10:28:35.245029Z","shell.execute_reply":"2022-03-12T10:31:27.933966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 883","metadata":{}},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/deberta-v3-large-5-folds-public/\"\n#     config_path= '../input/deberta-v3-large-5-folds-public/config.pth'\n#     model=\"microsoft/deberta-v3-large\"\n#     batch_size=32\n#     fc_dropout=0.2\n#     max_len=354\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:27.939276Z","iopub.execute_input":"2022-03-12T10:31:27.941099Z","iopub.status.idle":"2022-03-12T10:31:27.946568Z","shell.execute_reply.started":"2022-03-12T10:31:27.94105Z","shell.execute_reply":"2022-03-12T10:31:27.945947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n# tokenizer = DebertaV2TokenizerFast.from_pretrained(\"../input/deberta-tokenizer\")\n# CFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:27.950195Z","iopub.execute_input":"2022-03-12T10:31:27.952185Z","iopub.status.idle":"2022-03-12T10:31:27.964998Z","shell.execute_reply.started":"2022-03-12T10:31:27.95215Z","shell.execute_reply":"2022-03-12T10:31:27.964304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ScoringModel(nn.Module):\n#     def __init__(self, cfg, config_path=None, pretrained=False):\n#         super().__init__()\n#         self.cfg = cfg\n        \n#         if config_path is None:\n#             self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n#         else:\n#             self.config = torch.load(config_path)\n#         if pretrained:\n#             self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n#         else:\n#             self.model = AutoModel.from_config(self.config)\n#         self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n#         self.fc = nn.Linear(self.config.hidden_size, 1)\n#         self._init_weights(self.fc)\n        \n#     def _init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.bias is not None:\n#                 module.bias.data.zero_()\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n#         elif isinstance(module, nn.LayerNorm):\n#             module.bias.data.zero_()\n#             module.weight.data.fill_(1.0)\n        \n#     def feature(self, inputs):\n#         outputs = self.model(**inputs)\n#         last_hidden_states = outputs[0]\n        \n#         return last_hidden_states\n\n#     def forward(self, inputs):\n#         feature = self.feature(inputs)\n#         output = self.fc(self.fc_dropout(feature))\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:27.970506Z","iopub.execute_input":"2022-03-12T10:31:27.972347Z","iopub.status.idle":"2022-03-12T10:31:27.979139Z","shell.execute_reply.started":"2022-03-12T10:31:27.972312Z","shell.execute_reply":"2022-03-12T10:31:27.97846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions2 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:27.983982Z","iopub.execute_input":"2022-03-12T10:31:27.985076Z","iopub.status.idle":"2022-03-12T10:31:27.994187Z","shell.execute_reply.started":"2022-03-12T10:31:27.98504Z","shell.execute_reply":"2022-03-12T10:31:27.9935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 882","metadata":{}},{"cell_type":"code","source":"class CFG:\n    num_workers=4\n    path=[ \"../input/debertafclayerinite/microsoft-deberta-v3-large_fold0_best.pth\",\n          \"../input/debertafclayerinite/microsoft-deberta-v3-large_fold1_best.pth\",\n          \"../input/debertafclayerinite/microsoft-deberta-v3-large_fold2_best.pth\",\n          \"../input/debertafclayerinite/microsoft-deberta-v3-large_fold3_best.pth\",\n          \"../input/debertafclayerinite/microsoft-deberta-v3-large_fold4_best.pth\"\n         ]\n    config_path='../input/debertafclayerinite/config01.pth'\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:27.997363Z","iopub.execute_input":"2022-03-12T10:31:27.998274Z","iopub.status.idle":"2022-03-12T10:31:28.008694Z","shell.execute_reply.started":"2022-03-12T10:31:27.998237Z","shell.execute_reply":"2022-03-12T10:31:28.007969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ====================================================\n# Model\n# ====================================================\ndef init_params(module_lst):\n    for module in module_lst:\n        for param in module.parameters():\n            if param.dim() > 1:\n                torch.nn.init.xavier_uniform_(param)\n    return\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Sequential(\n            nn.Linear(self.config.hidden_size, 512),\n            nn.LayerNorm(512),\n            nn.Dropout(0.2),\n            nn.ReLU(),\n            nn.Linear(512, 1),\n        )\n        init_params([self.fc])\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(feature)\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:28.011581Z","iopub.execute_input":"2022-03-12T10:31:28.012777Z","iopub.status.idle":"2022-03-12T10:31:28.028925Z","shell.execute_reply.started":"2022-03-12T10:31:28.012681Z","shell.execute_reply":"2022-03-12T10:31:28.02823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\ntokenizer = DebertaV2TokenizerFast.from_pretrained(\"../input/deberta-tokenizer\")\nCFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:28.030222Z","iopub.execute_input":"2022-03-12T10:31:28.030507Z","iopub.status.idle":"2022-03-12T10:31:29.084073Z","shell.execute_reply.started":"2022-03-12T10:31:28.030471Z","shell.execute_reply":"2022-03-12T10:31:29.083333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\npredictions = []\nfor path in CFG.path:\n    model = CustomModel(CFG, config_path=CFG.config_path, pretrained=False)\n    state = torch.load(path,\n                           map_location=torch.device('cpu'))         \n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\npredictions2 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:31:29.08534Z","iopub.execute_input":"2022-03-12T10:31:29.085614Z","iopub.status.idle":"2022-03-12T10:33:54.639023Z","shell.execute_reply.started":"2022-03-12T10:31:29.085579Z","shell.execute_reply":"2022-03-12T10:33:54.637954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CFG:\n#     num_workers=4\n#     path=\"../input/deberta-v3-large-5-folds-public/\"\n#     config_path=path+'config.pth'\n#     model=\"microsoft/deberta-v3-large\"\n#     batch_size=32\n#     fc_dropout=0.2\n#     max_len=354\n#     seed=42\n#     n_fold=5\n#     trn_fold=[0, 1, 2, 3, 4]","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:33:54.644305Z","iopub.execute_input":"2022-03-12T10:33:54.644597Z","iopub.status.idle":"2022-03-12T10:33:54.653256Z","shell.execute_reply.started":"2022-03-12T10:33:54.644551Z","shell.execute_reply":"2022-03-12T10:33:54.652636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n# tokenizer = DebertaV2TokenizerFast.from_pretrained(\"../input/deberta-tokenizer\")\n# CFG.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:33:54.656949Z","iopub.execute_input":"2022-03-12T10:33:54.658861Z","iopub.status.idle":"2022-03-12T10:33:54.673243Z","shell.execute_reply.started":"2022-03-12T10:33:54.658824Z","shell.execute_reply":"2022-03-12T10:33:54.672391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ScoringModel(nn.Module):\n#     def __init__(self, cfg, config_path=None, pretrained=False):\n#         super().__init__()\n#         self.cfg = cfg\n        \n#         if config_path is None:\n#             self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n#         else:\n#             self.config = torch.load(config_path)\n#         if pretrained:\n#             self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n#         else:\n#             self.model = AutoModel.from_config(self.config)\n#         self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n#         self.fc = nn.Linear(self.config.hidden_size, 1)\n#         self._init_weights(self.fc)\n        \n#     def _init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.bias is not None:\n#                 module.bias.data.zero_()\n#         elif isinstance(module, nn.Embedding):\n#             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n#             if module.padding_idx is not None:\n#                 module.weight.data[module.padding_idx].zero_()\n#         elif isinstance(module, nn.LayerNorm):\n#             module.bias.data.zero_()\n#             module.weight.data.fill_(1.0)\n        \n#     def feature(self, inputs):\n#         outputs = self.model(**inputs)\n#         last_hidden_states = outputs[0]\n        \n#         return last_hidden_states\n\n#     def forward(self, inputs):\n#         feature = self.feature(inputs)\n#         output = self.fc(self.fc_dropout(feature))\n#         return output","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:33:54.676455Z","iopub.execute_input":"2022-03-12T10:33:54.67725Z","iopub.status.idle":"2022-03-12T10:33:54.688599Z","shell.execute_reply.started":"2022-03-12T10:33:54.677215Z","shell.execute_reply":"2022-03-12T10:33:54.687973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_dataset = TestDataset(CFG, test)\n# test_loader = DataLoader(test_dataset,\n#                          batch_size=CFG.batch_size,\n#                          shuffle=False,\n#                          num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n# predictions = []\n# for fold in CFG.trn_fold:\n#     model = ScoringModel(CFG, config_path=CFG.config_path, pretrained=False)\n    \n#     state = torch.load(CFG.path+f\"{CFG.model.split('/')[1]}_fold{fold}_best.pth\",\n#                            map_location=torch.device('cpu'))\n       \n#     model.load_state_dict(state['model'])\n#     prediction = inference_fn(test_loader, model, device)\n#     prediction = prediction.reshape((len(test), CFG.max_len))\n#     char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n#     predictions.append(char_probs)\n#     del model, state, prediction, char_probs\n#     gc.collect()\n#     torch.cuda.empty_cache()\n    \n# predictions3 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:33:54.691346Z","iopub.execute_input":"2022-03-12T10:33:54.693097Z","iopub.status.idle":"2022-03-12T10:33:54.714258Z","shell.execute_reply.started":"2022-03-12T10:33:54.693059Z","shell.execute_reply":"2022-03-12T10:33:54.713527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = ( predictions1 + predictions2 ) / 2\n# predictions = predictions1\nresults = get_results(predictions1, th=0.47) # best_th\nsubmission['location'] = results\ndisplay(submission.head())\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T10:33:54.718704Z","iopub.execute_input":"2022-03-12T10:33:54.719173Z","iopub.status.idle":"2022-03-12T10:33:54.759353Z","shell.execute_reply.started":"2022-03-12T10:33:54.719137Z","shell.execute_reply":"2022-03-12T10:33:54.758733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}