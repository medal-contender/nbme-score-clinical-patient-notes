{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Define Environment","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom pathlib import Path\n\ntransformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n\ninput_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n\nconvert_file = input_dir / \"convert_slow_tokenizer.py\"\nconversion_path = transformers_path/convert_file.name\n\nif conversion_path.exists():\n    conversion_path.unlink()\n\nshutil.copy(convert_file, transformers_path)\ndeberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n\nfor filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n    filepath = deberta_v2_path/filename\n    if filepath.exists():\n        filepath.unlink()\n\n    shutil.copy(input_dir/filename, filepath)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T01:39:40.392703Z","iopub.execute_input":"2022-03-18T01:39:40.393302Z","iopub.status.idle":"2022-03-18T01:39:40.446286Z","shell.execute_reply.started":"2022-03-18T01:39:40.393208Z","shell.execute_reply":"2022-03-18T01:39:40.445609Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 1. DeBERTA base (0.861)","metadata":{}},{"cell_type":"code","source":"''' DeBERTa Base'''\n\n# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport pathlib\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom glob import glob\nimport pathlib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# sys.path.append('../input/pickle5/pickle5-backport-master')\n# import pickle5 as pickle\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n# ** TODO **\n#### 1. fix YOUR_DATASET_PATH\n#### 2. fix Model Architecture\n#../input/nbme-deberta-base-baseline-train/microsoft-deberta-base_fold2_best.pth\n\nclass CFG:\n    YOUR_DATASET_PATH = \"../input/nbme-deberta-base-baseline-train/\"\n    \n    PKL_PATH = glob(os.path.join(YOUR_DATASET_PATH, '*.pkl'))[0]\n    CONFIG_PATH = glob(os.path.join(YOUR_DATASET_PATH, '*config.pth'))[0]\n    MODEL_PATHS = sorted(pathlib.Path(YOUR_DATASET_PATH).glob('*fold*.pth'))\n    \n    num_workers=0\n    model=\"microsoft/deberta-base\"\n    batch_size=24\n    fc_dropout=0.2\n    max_len=466\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \n\ntokenizer_path = '../input/nbme-deberta-base-baseline-train/'\nCFG.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path+'tokenizer/')\n\n\n# ====================================================\n# Model\n# ====================================================\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout = nn.Dropout(cfg.fc_dropout)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output = self.fc(self.fc_dropout(feature))\n        return output\n\n\n# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)\n\ndef create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n    \ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\n\ndef get_char_probs_for_ensemble(texts, predictions, tokenizer):\n    results = np.zeros((len(predictions), 5000))\n    \n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        before_offset_2 = 0\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (pred, offset_mapping, seq_ids) in enumerate(zip(prediction, encoded['offset_mapping'], encoded.sequence_ids())):\n            if seq_ids != 0:\n                continue\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            if start!=before_offset_2:\n                start=before_offset_2\n            before_offset_2=end\n            results[i][start:end] = pred\n    return results\n\n# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n\n# ====================================================\n# oof\n# ====================================================\ntry:\n    with open(CFG.pkl_path, \"rb\") as fh:\n        data = pickle.load(fh)\n    data = data.reset_index(drop=True)\n    oof = data\n\n    truths = create_labels_for_scoring(oof)\n    char_probs = get_char_probs(oof['pn_history'].values,\n                                oof[[i for i in range(CFG.max_len)]].values, \n                                CFG.tokenizer)\n    best_th = 0.5\n    best_score = 0.\n    for th in np.arange(0.45, 0.55, 0.01):\n        th = np.round(th, 2)\n        results = get_results(char_probs, th=th)\n        preds = get_predictions(results)\n        score = get_score(preds, truths)\n        if best_score < score:\n            best_th = th\n            best_score = score\n        LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n    LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")\nexcept:\n    best_th=0.5\nprint(f\"[[[[[ YOUR BEST_TH : {best_th} ]]]]]\")\n\n\n# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\ntest = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n\n\n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        return inputs\n\n\n# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=0, pin_memory=True, drop_last=False)\npredictions = []\nfor path in tqdm(CFG.MODEL_PATHS, total=len(CFG.MODEL_PATHS)):\n    model = CustomModel(CFG, config_path=CFG.CONFIG_PATH, pretrained=False)\n    state = torch.load(path,\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs_for_ensemble(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\nchar_predictions0 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T01:39:40.577688Z","iopub.execute_input":"2022-03-18T01:39:40.578247Z","iopub.status.idle":"2022-03-18T01:40:30.913791Z","shell.execute_reply.started":"2022-03-18T01:39:40.578213Z","shell.execute_reply":"2022-03-18T01:40:30.913040Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 2. DeBERTA v3 large Deepshare (0.884)","metadata":{}},{"cell_type":"code","source":"''' DeBERTa v3 large '''\n''' You Should run [DeBERTa v3 large] FIRST!! '''\n\n    \n# ====================================================\n# Library\n# ====================================================\nimport os\nimport gc\nimport pathlib\nimport re\nimport ast\nimport sys\nimport copy\nimport json\nimport time\nimport math\nimport string\nimport pickle\nimport random\nimport joblib\nimport itertools\nfrom glob import glob\nimport pathlib\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# sys.path.append('../input/pickle5/pickle5-backport-master')\n# import pickle5 as pickle\nimport scipy as sp\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\nimport torch.nn.functional as F\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.utils.data import DataLoader, Dataset\n# os.system('pip uninstall -y transformers')\n# os.system('python -m pip install --no-index --find-links=../input/nbme-pip-wheels transformers')\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n%env TOKENIZERS_PARALLELISM=true\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n\n\n# ** TODO **\n#### 1. fix YOUR_DATASET_PATH\n#### 2. fix Model Architecture\n#../input/nbme-deberta-base-baseline-train/microsoft-deberta-base_fold2_best.pth\n\nclass CFG:\n    YOUR_DATASET_PATH = \"../input/deberta-deepshare\"\n    \n    PKL_PATH = glob(os.path.join(YOUR_DATASET_PATH, '*.pkl'))[0]\n    #CONFIG_PATH = glob(os.path.join(YOUR_DATASET_PATH, '*config.pth'))[0]\n    MODEL_PATHS = sorted(pathlib.Path(YOUR_DATASET_PATH).glob('*fold*.pth'))\n    \n    num_workers=0\n    model=\"../input/deberta-v3-large/deberta-v3-large\"\n    batch_size=32\n    fc_dropout=0.2\n    max_len=354\n    seed=42\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    \n\n    \n# tokenizer_path = '../input/nbme-deberta-base-baseline-train/'\n# CFG.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path+'tokenizer/')\nfrom transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\ntokenizer = DebertaV2TokenizerFast.from_pretrained('../input/get-token/tokenizer')\nCFG.tokenizer = tokenizer\n\n# ====================================================\n# Model\n# ====================================================\n# deepshare model\nclass DeepShareModel(nn.Module):\n    def __init__(self, cfg, config_path=None, pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n        else:\n            self.config = torch.load(config_path)\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        self.fc_dropout_0 = nn.Dropout(0.1)\n        self.fc_dropout_1 = nn.Dropout(0.2)\n        self.fc_dropout_2 = nn.Dropout(0.3)\n        self.fc_dropout_3 = nn.Dropout(0.4)\n        self.fc_dropout_4 = nn.Dropout(0.5)\n        self.fc = nn.Linear(self.config.hidden_size, 1)\n        self._init_weights(self.fc)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        last_hidden_states = outputs[0]\n        return last_hidden_states\n\n    def forward(self, inputs):\n        feature = self.feature(inputs)\n        output_0 = self.fc(self.fc_dropout_0(feature))\n        output_1 = self.fc(self.fc_dropout_1(feature))\n        output_2 = self.fc(self.fc_dropout_2(feature))\n        output_3 = self.fc(self.fc_dropout_3(feature))\n        output_4 = self.fc(self.fc_dropout_4(feature))\n        output = (output_0 + output_1 + output_2 + output_3 + output_4) / 5\n        return output\n\n\n# From https://www.kaggle.com/theoviel/evaluation-metric-folds-baseline\ndef micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on binary arrays.\n\n    Args:\n        preds (list of lists of ints): Predictions.\n        truths (list of lists of ints): Ground truths.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    # Micro : aggregating over all instances\n    preds = np.concatenate(preds)\n    truths = np.concatenate(truths)\n    return f1_score(truths, preds)\n\ndef spans_to_binary(spans, length=None):\n    \"\"\"\n    Converts spans to a binary array indicating whether each character is in the span.\n\n    Args:\n        spans (list of lists of two ints): Spans.\n\n    Returns:\n        np array [length]: Binarized spans.\n    \"\"\"\n    length = np.max(spans) if length is None else length\n    binary = np.zeros(length)\n    for start, end in spans:\n        binary[start:end] = 1\n    return binary\n\ndef span_micro_f1(preds, truths):\n    \"\"\"\n    Micro f1 on spans.\n\n    Args:\n        preds (list of lists of two ints): Prediction spans.\n        truths (list of lists of two ints): Ground truth spans.\n\n    Returns:\n        float: f1 score.\n    \"\"\"\n    bin_preds = []\n    bin_truths = []\n    for pred, truth in zip(preds, truths):\n        if not len(pred) and not len(truth):\n            continue\n        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n        bin_preds.append(spans_to_binary(pred, length))\n        bin_truths.append(spans_to_binary(truth, length))\n    return micro_f1(bin_preds, bin_truths)\n\ndef create_labels_for_scoring(df):\n    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n    for i in range(len(df)):\n        lst = df.loc[i, 'location']\n        if lst:\n            new_lst = ';'.join(lst)\n            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n    # create labels\n    truths = []\n    for location_list in df['location_for_create_labels'].values:\n        truth = []\n        if len(location_list) > 0:\n            location = location_list[0]\n            for loc in [s.split() for s in location.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                truth.append([start, end])\n        truths.append(truth)\n    return truths\n    \ndef get_char_probs(texts, predictions, tokenizer):\n    results = [np.zeros(len(t)) for t in texts]\n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            results[i][start:end] = pred\n    return results\n\ndef get_results(char_probs, th=0.5):\n    results = []\n    for char_prob in char_probs:\n        result = np.where(char_prob >= th)[0] + 1\n        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n        result = [f\"{min(r)} {max(r)}\" for r in result]\n        result = \";\".join(result)\n        results.append(result)\n    return results\n\ndef get_predictions(results):\n    predictions = []\n    for result in results:\n        prediction = []\n        if result != \"\":\n            for loc in [s.split() for s in result.split(';')]:\n                start, end = int(loc[0]), int(loc[1])\n                prediction.append([start, end])\n        predictions.append(prediction)\n    return predictions\n\ndef get_char_probs_for_ensemble(texts, predictions, tokenizer):\n    results = np.zeros((len(predictions), 5000))\n    \n    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n        before_offset_2 = 0\n        encoded = tokenizer(text, \n                            add_special_tokens=True,\n                            return_offsets_mapping=True)\n        for idx, (pred, offset_mapping, seq_ids) in enumerate(zip(prediction, encoded['offset_mapping'], encoded.sequence_ids())):\n            if seq_ids != 0:\n                continue\n            start = offset_mapping[0]\n            end = offset_mapping[1]\n            if start!=before_offset_2:\n                start=before_offset_2\n            before_offset_2=end\n            results[i][start:end] = pred\n    return results\n\n# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    score = span_micro_f1(y_true, y_pred)\n    return score\n\n\ndef get_logger(filename='inference'):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = get_logger()\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)\n\n\n# ====================================================\n# oof\n# ====================================================\ntry:\n    with open(CFG.pkl_path, \"rb\") as fh:\n        data = pickle.load(fh)\n    data = data.reset_index(drop=True)\n    oof = data\n\n    truths = create_labels_for_scoring(oof)\n    char_probs = get_char_probs(oof['pn_history'].values,\n                                oof[[i for i in range(CFG.max_len)]].values, \n                                CFG.tokenizer)\n    best_th = 0.5\n    best_score = 0.\n    for th in np.arange(0.45, 0.55, 0.01):\n        th = np.round(th, 2)\n        results = get_results(char_probs, th=th)\n        preds = get_predictions(results)\n        score = get_score(preds, truths)\n        if best_score < score:\n            best_th = th\n            best_score = score\n        LOGGER.info(f\"th: {th}  score: {score:.5f}\")\n    LOGGER.info(f\"best_th: {best_th}  score: {best_score:.5f}\")\nexcept:\n    best_th=0.5\nprint(f\"[[[[[ YOUR BEST_TH : {best_th} ]]]]]\")\n\n\n# ====================================================\n# Data Loading\n# ====================================================\ntest = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\nsubmission = pd.read_csv('../input/nbme-score-clinical-patient-notes/sample_submission.csv')\nfeatures = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\ndef preprocess_features(features):\n    features.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n    return features\nfeatures = preprocess_features(features)\npatient_notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')\n\ntest = test.merge(features, on=['feature_num', 'case_num'], how='left')\ntest = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n\n\n# ====================================================\n# Dataset\n# ====================================================\ndef prepare_input(cfg, text, feature_text):\n    inputs = cfg.tokenizer(text, feature_text, \n                           add_special_tokens=True,\n                           max_length=CFG.max_len,\n                           padding=\"max_length\",\n                           return_offsets_mapping=False)\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long)\n    return inputs\n\n\nclass TestDataset(Dataset):\n    def __init__(self, cfg, df):\n        self.cfg = cfg\n        self.feature_texts = df['feature_text'].values\n        self.pn_historys = df['pn_history'].values\n\n    def __len__(self):\n        return len(self.feature_texts)\n\n    def __getitem__(self, item):\n        inputs = prepare_input(self.cfg, \n                               self.pn_historys[item], \n                               self.feature_texts[item])\n        return inputs\n\n\n# ====================================================\n# inference\n# ====================================================\ndef inference_fn(test_loader, model, device):\n    preds = []\n    model.eval()\n    model.to(device)\n    tk0 = tqdm(test_loader, total=len(test_loader))\n    for inputs in tk0:\n        for k, v in inputs.items():\n            inputs[k] = v.to(device)\n        with torch.no_grad():\n            y_preds = model(inputs)\n        preds.append(y_preds.sigmoid().to('cpu').numpy())\n    predictions = np.concatenate(preds)\n    return predictions\n\n\ntest_dataset = TestDataset(CFG, test)\ntest_loader = DataLoader(test_dataset,\n                         batch_size=CFG.batch_size,\n                         shuffle=False,\n                         num_workers=0, pin_memory=True, drop_last=False)\npredictions = []\nfor path in tqdm(CFG.MODEL_PATHS, total=len(CFG.MODEL_PATHS)):\n    model = DeepShareModel(CFG, config_path=None, pretrained=False)\n    state = torch.load(path,\n                       map_location=torch.device('cpu'))\n    model.load_state_dict(state['model'])\n    prediction = inference_fn(test_loader, model, device)\n    prediction = prediction.reshape((len(test), CFG.max_len))\n    char_probs = get_char_probs_for_ensemble(test['pn_history'].values, prediction, CFG.tokenizer)\n    predictions.append(char_probs)\n    del model, state, prediction, char_probs; gc.collect()\n    torch.cuda.empty_cache()\nchar_predictions1 = np.mean(predictions, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-18T01:40:30.916538Z","iopub.execute_input":"2022-03-18T01:40:30.916813Z","iopub.status.idle":"2022-03-18T01:42:28.313121Z","shell.execute_reply.started":"2022-03-18T01:40:30.916776Z","shell.execute_reply":"2022-03-18T01:42:28.312372Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 2. RoBERTa large baseline (0.880)","metadata":{}},{"cell_type":"code","source":"''' RoBERTa Large'''\nimport os\nimport re\nimport ast\nimport json\nimport glob\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nDATA_PATH = \"../input/nbme-score-clinical-patient-notes/\"\nOUT_PATH = \"../input/nbme-roberta-large/\"\nWEIGHTS_FOLDER = \"../input/nbme-roberta-large/\"\n\nNUM_WORKERS = 2\n\ndef process_feature_text(text):\n    text = re.sub('I-year', '1-year', text)\n    text = re.sub('-OR-', \" or \", text)\n    text = re.sub('-', ' ', text)\n    return text\n\n\ndef clean_spaces(txt):\n    txt = re.sub('\\n', ' ', txt)\n    txt = re.sub('\\t', ' ', txt)\n    txt = re.sub('\\r', ' ', txt)\n    txt = re.sub(r'\\s+', ' ', txt)\n    return txt\n\n\ndef load_and_prepare_test(root=\"\"):\n    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n    features = pd.read_csv(root + \"features.csv\")\n    df = pd.read_csv(root + \"test.csv\")\n\n    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n\n    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n\n    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n    #df['clean_text'] = df['pn_history'].apply(clean_spaces)\n    df['clean_text'] = df['pn_history']\n\n    df['target'] = \"\"\n    return df\n\nimport itertools\n\n\ndef token_pred_to_char_pred(token_pred, offsets):\n    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n    for i in range(len(token_pred)):\n        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n        char_pred[s:e] = token_pred[i]\n\n        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n            s += 1\n            char_pred[s: e, 1], char_pred[s: e, 2] = (\n                np.max(char_pred[s: e, 1:], 1),\n                np.min(char_pred[s: e, 1:], 1),\n            )\n\n    return char_pred\n\n\ndef labels_to_sub(labels):\n    all_spans = []\n    for label in labels:\n        indices = np.where(label > 0)[0]\n        indices_grouped = [\n            list(g) for _, g in itertools.groupby(\n                indices, key=lambda n, c=itertools.count(): n - next(c)\n            )\n        ]\n\n        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n        all_spans.append(\";\".join(spans))\n    return all_spans\n\n\ndef char_target_to_span(char_target):\n    spans = []\n    start, end = 0, 0\n    for i in range(len(char_target)):\n        if char_target[i] == 1 and char_target[i - 1] == 0:\n            if end:\n                spans.append([start, end])\n            start = i\n            end = i + 1\n        elif char_target[i] == 1:\n            end = i + 1\n        else:\n            if end:\n                spans.append([start, end])\n            start, end = 0, 0\n    return spans\n\n\nimport numpy as np\nfrom transformers import AutoTokenizer\n\n\ndef get_tokenizer(name, precompute=False, df=None, folder=None):\n    if folder is None:\n        tokenizer = AutoTokenizer.from_pretrained(name)\n    else:\n        tokenizer = AutoTokenizer.from_pretrained(folder)\n\n    tokenizer.name = name\n    tokenizer.special_tokens = {\n        \"sep\": tokenizer.sep_token_id,\n        \"cls\": tokenizer.cls_token_id,\n        \"pad\": tokenizer.pad_token_id,\n    }\n\n    if precompute:\n        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n    else:\n        tokenizer.precomputed = None\n\n    return tokenizer\n\n\ndef precompute_tokens(df, tokenizer):\n    feature_texts = df[\"feature_text\"].unique()\n\n    ids = {}\n    offsets = {}\n\n    for feature_text in feature_texts:\n        encoding = tokenizer(\n            feature_text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[feature_text] = encoding[\"input_ids\"]\n        offsets[feature_text] = encoding[\"offset_mapping\"]\n\n    texts = df[\"clean_text\"].unique()\n\n    for text in texts:\n        encoding = tokenizer(\n            text,\n            return_token_type_ids=True,\n            return_offsets_mapping=True,\n            return_attention_mask=False,\n            add_special_tokens=False,\n        )\n        ids[text] = encoding[\"input_ids\"]\n        offsets[text] = encoding[\"offset_mapping\"]\n\n    return {\"ids\": ids, \"offsets\": offsets}\n\n\ndef encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n    tokens = tokenizer.special_tokens\n\n    # Input ids\n    if \"roberta\" in tokenizer.name:\n        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n    else:\n        qa_sep = [tokens[\"sep\"]]\n\n    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n    n_question_tokens = len(input_ids)\n\n    input_ids += precomputed[\"ids\"][text]\n    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n\n    # Token type ids\n    if \"roberta\" not in tokenizer.name:\n        token_type_ids = np.ones(len(input_ids))\n        token_type_ids[:n_question_tokens] = 0\n        token_type_ids = token_type_ids.tolist()\n    else:\n        token_type_ids = [0] * len(input_ids)\n\n    # Offsets\n    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n    offsets = offsets[: max_len - 1] + [(0, 0)]\n\n    # Padding\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        offsets = offsets + ([(0, 0)] * padding_length)\n\n    encoding = {\n        \"input_ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"offset_mapping\": offsets,\n    }\n\n    return encoding\n\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n\nclass PatientNoteDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.df = df\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n\n        self.texts = df['clean_text'].values\n        self.feature_text = df['feature_text'].values\n        self.char_targets = df['target'].values.tolist()\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        feature_text = self.feature_text[idx]\n        char_target = self.char_targets[idx]\n\n        # Tokenize\n        if self.tokenizer.precomputed is None:\n            encoding = self.tokenizer(\n                feature_text,\n                text,\n                return_token_type_ids=True,\n                return_offsets_mapping=True,\n                return_attention_mask=False,\n                truncation=\"only_second\",\n                max_length=self.max_len,\n                padding='max_length',\n            )\n            raise NotImplementedError(\"fix issues with question offsets\")\n        else:\n            encoding = encodings_from_precomputed(\n                feature_text,\n                text,\n                self.tokenizer.precomputed,\n                self.tokenizer,\n                max_len=self.max_len\n            )\n\n        return {\n            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n            \"target\": torch.tensor([0], dtype=torch.float),\n            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n            \"text\": text,\n        }\n\n    def __len__(self):\n        return len(self.texts)\n\n    \n    \nimport torch\nimport transformers\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModel\n\n\nclass NERTransformer(nn.Module):\n    def __init__(\n        self,\n        model,\n        num_classes=1,\n        config_file=None,\n        pretrained=True,\n    ):\n        super().__init__()\n        self.name = model\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        transformers.logging.set_verbosity_error()\n\n        if config_file is None:\n            config = AutoConfig.from_pretrained(model, output_hidden_states=True)\n        else:\n            config = torch.load(config_file)\n\n        if pretrained:\n            self.transformer = AutoModel.from_pretrained(model, config=config)\n        else:\n            self.transformer = AutoModel.from_config(config)\n\n        self.nb_features = config.hidden_size\n\n#         self.cnn = nn.Identity()\n        self.logits = nn.Linear(self.nb_features, num_classes)\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n\n        logits = self.logits(features)\n\n        return logits\n    \nimport torch\n\ndef load_model_weights(model, filename, verbose=1, cp_folder=\"\", strict=True):\n    \"\"\"\n    Loads the weights of a PyTorch model. The exception handles cpu/gpu incompatibilities.\n\n    Args:\n        model (torch model): Model to load the weights to.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to load from. Defaults to \"\".\n        strict (bool, optional): Whether to allow missing/additional keys. Defaults to False.\n\n    Returns:\n        torch model: Model with loaded weights.\n    \"\"\"\n    if verbose:\n        print(f\"\\n -> Loading weights from {os.path.join(cp_folder,filename)}\\n\")\n\n    try:\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n    except RuntimeError:\n        model.encoder.fc = torch.nn.Linear(model.nb_ft, 1)\n        model.load_state_dict(\n            torch.load(os.path.join(cp_folder, filename), map_location=\"cpu\"),\n            strict=strict,\n        )\n\n    return model\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm.notebook import tqdm\n\n\ndef predict(model, dataset, data_config, activation=\"softmax\"):\n    \"\"\"\n    Usual predict torch function\n    \"\"\"\n    model.eval()\n\n    loader = DataLoader(\n        dataset,\n        batch_size=data_config['val_bs'],\n        shuffle=False,\n        num_workers=NUM_WORKERS,\n        pin_memory=True,\n    )\n\n    preds = []\n    with torch.no_grad():\n        for data in tqdm(loader):\n            ids, token_type_ids = data[\"ids\"], data[\"token_type_ids\"]\n\n            y_pred = model(ids.cuda(), token_type_ids.cuda())\n\n            if activation == \"sigmoid\":\n                y_pred = y_pred.sigmoid()\n            elif activation == \"softmax\":\n                y_pred = y_pred.softmax(-1)\n\n            preds += [\n                token_pred_to_char_pred(y, offsets) for y, offsets\n                in zip(y_pred.detach().cpu().numpy(), data[\"offsets\"].numpy())\n            ]\n\n    return preds\n\ndef inference_test(df, exp_folder, config, cfg_folder=None):\n    preds = []\n\n    if cfg_folder is not None:\n        model_config_file = cfg_folder + config.name.split('/')[-1] + \"/config.pth\"\n        tokenizer_folder = cfg_folder + config.name.split('/')[-1] + \"/tokenizers/\"\n    else:\n        model_config_file, tokenizer_folder = None, None\n\n    tokenizer = get_tokenizer(\n        config.name, precompute=config.precompute_tokens, df=df, folder=tokenizer_folder\n    )\n\n    dataset = PatientNoteDataset(\n        df,\n        tokenizer,\n        max_len=config.max_len,\n    )\n\n    model = NERTransformer(\n        config.name,\n        num_classes=config.num_classes,\n        config_file=model_config_file,\n        pretrained=False\n    ).cuda()\n    model.zero_grad()\n\n    weights = sorted(glob.glob(exp_folder + \"*.pt\"))\n    for weight in weights:\n        model = load_model_weights(model, weight)\n\n        pred = predict(\n            model,\n            dataset,\n            data_config=config.data_config,\n            activation=config.loss_config[\"activation\"]\n        )\n        preds.append(pred)\n\n    return preds\n\nclass Config:\n    # Architecture\n    name = \"roberta-large\"\n    num_classes = 1\n\n    # Texts\n    max_len = 310\n    precompute_tokens = True\n\n    # Training    \n    loss_config = {\n        \"activation\": \"sigmoid\",\n    }\n\n    data_config = {\n        \"val_bs\": 16 if \"large\" in name else 32,\n        \"pad_token\": 1 if \"roberta\" in name else 0,\n    }\n\n    verbose = 1\n    \ndf_test = load_and_prepare_test(root=DATA_PATH)\npreds = inference_test(\n    df_test,\n    WEIGHTS_FOLDER,\n    Config,\n    cfg_folder=OUT_PATH\n)[0]\ndf_test['preds'] = preds\ndf_test['preds'] = df_test.apply(lambda x: x['preds'][:len(x['clean_text'])], 1)\n\ndef post_process_spaces(preds):\n    char_preds = np.zeros((5000))\n    \n    for i in np.where(preds==0)[0]:\n        if i==len(preds)-1:\n            preds[i]=0\n        else:\n            preds[i] = preds[i+1]\n            \n    padding = np.zeros(len(char_preds) - len(preds))\n    preds = np.concatenate([preds.flatten(), padding])\n    return preds\n\n    \ndf_test['preds_pp'] = df_test.apply(lambda x: post_process_spaces(x['preds']), axis=1)\npredictions = []\nfor i, pred in df_test.iterrows():\n    predictions.append(pred['preds_pp'])\nchar_predictions2= predictions","metadata":{"execution":{"iopub.status.busy":"2022-03-18T01:43:53.145695Z","iopub.execute_input":"2022-03-18T01:43:53.145994Z","iopub.status.idle":"2022-03-18T01:44:00.704817Z","shell.execute_reply.started":"2022-03-18T01:43:53.145958Z","shell.execute_reply":"2022-03-18T01:44:00.704046Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"# DeBERTA-base(0.861) | DeBERTA-v3-large(0.884) | RoBERTa-large(0.880)\npreds_arr = np.array([char_predictions0, char_predictions1, char_predictions2]) \nset_list = np.array([0.2, 1.5, 1.2]) \n\npreds = np.average(preds_arr, axis=0, weights=set_list, returned=False)\n\nresults = get_results(preds, th=0.5)\nsubmission['location'] = results\nsubmission[['id', 'location']].to_csv('submission.csv', index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-18T01:53:52.852053Z","iopub.execute_input":"2022-03-18T01:53:52.852854Z","iopub.status.idle":"2022-03-18T01:53:52.868415Z","shell.execute_reply.started":"2022-03-18T01:53:52.852800Z","shell.execute_reply":"2022-03-18T01:53:52.867621Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}