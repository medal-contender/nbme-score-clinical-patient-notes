{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69ee2fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:57.682024Z",
     "iopub.status.busy": "2022-03-25T14:04:57.680528Z",
     "iopub.status.idle": "2022-03-25T14:04:57.707746Z",
     "shell.execute_reply": "2022-03-25T14:04:57.708196Z",
     "shell.execute_reply.started": "2022-03-25T14:00:40.958923Z"
    },
    "papermill": {
     "duration": 0.049454,
     "end_time": "2022-03-25T14:04:57.708404",
     "exception": false,
     "start_time": "2022-03-25T14:04:57.658950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    \n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b7e8a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:57.747635Z",
     "iopub.status.busy": "2022-03-25T14:04:57.746902Z",
     "iopub.status.idle": "2022-03-25T14:04:57.806355Z",
     "shell.execute_reply": "2022-03-25T14:04:57.805931Z",
     "shell.execute_reply.started": "2022-03-25T14:00:44.109415Z"
    },
    "papermill": {
     "duration": 0.079149,
     "end_time": "2022-03-25T14:04:57.806482",
     "exception": false,
     "start_time": "2022-03-25T14:04:57.727333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38f03e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:57.842541Z",
     "iopub.status.busy": "2022-03-25T14:04:57.841530Z",
     "iopub.status.idle": "2022-03-25T14:04:57.850834Z",
     "shell.execute_reply": "2022-03-25T14:04:57.850385Z",
     "shell.execute_reply.started": "2022-03-25T14:00:48.202413Z"
    },
    "papermill": {
     "duration": 0.027897,
     "end_time": "2022-03-25T14:04:57.850953",
     "exception": false,
     "start_time": "2022-03-25T14:04:57.823056",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_feature_text(text):\n",
    "    text = re.sub('I-year', '1-year', text)\n",
    "    text = re.sub('-OR-', \" or \", text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_spaces(txt):\n",
    "    txt = re.sub('\\n', ' ', txt)\n",
    "    txt = re.sub('\\t', ' ', txt)\n",
    "    txt = re.sub('\\r', ' ', txt)\n",
    "#     txt = re.sub(r'\\s+', ' ', txt)\n",
    "    return txt\n",
    "\n",
    "\n",
    "def load_and_prepare_test(root=\"\"):\n",
    "    patient_notes = pd.read_csv(root + \"patient_notes.csv\")\n",
    "    features = pd.read_csv(root + \"features.csv\")\n",
    "    df = pd.read_csv(root + \"test.csv\")\n",
    "\n",
    "    df = df.merge(features, how=\"left\", on=[\"case_num\", \"feature_num\"])\n",
    "    df = df.merge(patient_notes, how=\"left\", on=['case_num', 'pn_num'])\n",
    "\n",
    "    df['pn_history'] = df['pn_history'].apply(lambda x: x.strip())\n",
    "    df['feature_text'] = df['feature_text'].apply(process_feature_text)\n",
    "\n",
    "    df['feature_text'] = df['feature_text'].apply(clean_spaces)\n",
    "    df['clean_text'] = df['pn_history'].apply(clean_spaces)\n",
    "\n",
    "    df['target'] = \"\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba8f946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:57.894129Z",
     "iopub.status.busy": "2022-03-25T14:04:57.889848Z",
     "iopub.status.idle": "2022-03-25T14:04:57.896343Z",
     "shell.execute_reply": "2022-03-25T14:04:57.895952Z",
     "shell.execute_reply.started": "2022-03-25T14:00:52.314640Z"
    },
    "papermill": {
     "duration": 0.02964,
     "end_time": "2022-03-25T14:04:57.896452",
     "exception": false,
     "start_time": "2022-03-25T14:04:57.866812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "def token_pred_to_char_pred(token_pred, offsets):\n",
    "    char_pred = np.zeros((np.max(offsets), token_pred.shape[1]))\n",
    "    for i in range(len(token_pred)):\n",
    "        s, e = int(offsets[i][0]), int(offsets[i][1])  # start, end\n",
    "        char_pred[s:e] = token_pred[i]\n",
    "\n",
    "        if token_pred.shape[1] == 3:  # following characters cannot be tagged as start\n",
    "            s += 1\n",
    "            char_pred[s: e, 1], char_pred[s: e, 2] = (\n",
    "                np.max(char_pred[s: e, 1:], 1),\n",
    "                np.min(char_pred[s: e, 1:], 1),\n",
    "            )\n",
    "\n",
    "    return char_pred\n",
    "\n",
    "\n",
    "def labels_to_sub(labels):\n",
    "    all_spans = []\n",
    "    for label in labels:\n",
    "        indices = np.where(label > 0)[0]\n",
    "        indices_grouped = [\n",
    "            list(g) for _, g in itertools.groupby(\n",
    "                indices, key=lambda n, c=itertools.count(): n - next(c)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n",
    "        all_spans.append(\";\".join(spans))\n",
    "    return all_spans\n",
    "\n",
    "\n",
    "def char_target_to_span(char_target):\n",
    "    spans = []\n",
    "    start, end = 0, 0\n",
    "    for i in range(len(char_target)):\n",
    "        if char_target[i] == 1 and char_target[i - 1] == 0:\n",
    "            if end:\n",
    "                spans.append([start, end])\n",
    "            start = i\n",
    "            end = i + 1\n",
    "        elif char_target[i] == 1:\n",
    "            end = i + 1\n",
    "        else:\n",
    "            if end:\n",
    "                spans.append([start, end])\n",
    "            start, end = 0, 0\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "309c8700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:57.943531Z",
     "iopub.status.busy": "2022-03-25T14:04:57.942992Z",
     "iopub.status.idle": "2022-03-25T14:04:59.820095Z",
     "shell.execute_reply": "2022-03-25T14:04:59.819504Z",
     "shell.execute_reply.started": "2022-03-25T14:00:55.422275Z"
    },
    "papermill": {
     "duration": 1.907632,
     "end_time": "2022-03-25T14:04:59.820239",
     "exception": false,
     "start_time": "2022-03-25T14:04:57.912607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def get_tokenizer(name, precompute=False, df=None, folder=None):\n",
    "    if folder is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(folder)\n",
    "\n",
    "    tokenizer.name = name\n",
    "    tokenizer.special_tokens = {\n",
    "        \"sep\": tokenizer.sep_token_id,\n",
    "        \"cls\": tokenizer.cls_token_id,\n",
    "        \"pad\": tokenizer.pad_token_id,\n",
    "    }\n",
    "\n",
    "    if precompute:\n",
    "        tokenizer.precomputed = precompute_tokens(df, tokenizer)\n",
    "    else:\n",
    "        tokenizer.precomputed = None\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def precompute_tokens(df, tokenizer):\n",
    "    feature_texts = df[\"feature_text\"].unique()\n",
    "\n",
    "    ids = {}\n",
    "    offsets = {}\n",
    "\n",
    "    for feature_text in feature_texts:\n",
    "        encoding = tokenizer(\n",
    "            feature_text,\n",
    "            return_token_type_ids=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        ids[feature_text] = encoding[\"input_ids\"]\n",
    "        offsets[feature_text] = encoding[\"offset_mapping\"]\n",
    "\n",
    "    texts = df[\"clean_text\"].unique()\n",
    "\n",
    "    for text in texts:\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            return_token_type_ids=True,\n",
    "            return_offsets_mapping=True,\n",
    "            return_attention_mask=False,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        ids[text] = encoding[\"input_ids\"]\n",
    "        offsets[text] = encoding[\"offset_mapping\"]\n",
    "\n",
    "    return {\"ids\": ids, \"offsets\": offsets}\n",
    "\n",
    "\n",
    "def encodings_from_precomputed(feature_text, text, precomputed, tokenizer, max_len=300):\n",
    "    tokens = tokenizer.special_tokens\n",
    "\n",
    "    # Input ids\n",
    "    if \"roberta\" in tokenizer.name:\n",
    "        qa_sep = [tokens[\"sep\"], tokens[\"sep\"]]\n",
    "    else:\n",
    "        qa_sep = [tokens[\"sep\"]]\n",
    "\n",
    "    input_ids = [tokens[\"cls\"]] + precomputed[\"ids\"][feature_text] + qa_sep\n",
    "    n_question_tokens = len(input_ids)\n",
    "\n",
    "    input_ids += precomputed[\"ids\"][text]\n",
    "    input_ids = input_ids[: max_len - 1] + [tokens[\"sep\"]]\n",
    "\n",
    "    # Token type ids\n",
    "    if \"roberta\" not in tokenizer.name:\n",
    "        token_type_ids = np.ones(len(input_ids))\n",
    "        token_type_ids[:n_question_tokens] = 0\n",
    "        token_type_ids = token_type_ids.tolist()\n",
    "    else:\n",
    "        token_type_ids = [0] * len(input_ids)\n",
    "\n",
    "    # Offsets\n",
    "    offsets = [(0, 0)] * n_question_tokens + precomputed[\"offsets\"][text]\n",
    "    offsets = offsets[: max_len - 1] + [(0, 0)]\n",
    "\n",
    "    # Padding\n",
    "    padding_length = max_len - len(input_ids)\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "        offsets = offsets + ([(0, 0)] * padding_length)\n",
    "\n",
    "    encoding = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"token_type_ids\": token_type_ids,\n",
    "        \"offset_mapping\": offsets,\n",
    "    }\n",
    "\n",
    "    return encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55dd814c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:59.864616Z",
     "iopub.status.busy": "2022-03-25T14:04:59.863796Z",
     "iopub.status.idle": "2022-03-25T14:04:59.865527Z",
     "shell.execute_reply": "2022-03-25T14:04:59.865975Z",
     "shell.execute_reply.started": "2022-03-25T14:00:58.465855Z"
    },
    "papermill": {
     "duration": 0.02932,
     "end_time": "2022-03-25T14:04:59.866102",
     "exception": false,
     "start_time": "2022-03-25T14:04:59.836782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class PatientNoteDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.texts = df['clean_text'].values\n",
    "        self.feature_text = df['feature_text'].values\n",
    "        self.char_targets = df['target'].values.tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        feature_text = self.feature_text[idx]\n",
    "        char_target = self.char_targets[idx]\n",
    "\n",
    "        # Tokenize\n",
    "        if self.tokenizer.precomputed is None:\n",
    "            encoding = self.tokenizer(\n",
    "                feature_text,\n",
    "                text,\n",
    "                return_token_type_ids=True,\n",
    "                return_offsets_mapping=True,\n",
    "                return_attention_mask=False,\n",
    "                truncation=\"only_second\",\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "            )\n",
    "            raise NotImplementedError(\"fix issues with question offsets\")\n",
    "        else:\n",
    "            encoding = encodings_from_precomputed(\n",
    "                feature_text,\n",
    "                text,\n",
    "                self.tokenizer.precomputed,\n",
    "                self.tokenizer,\n",
    "                max_len=self.max_len\n",
    "            )\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(encoding[\"input_ids\"], dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(encoding[\"token_type_ids\"], dtype=torch.long),\n",
    "            \"target\": torch.tensor([0], dtype=torch.float),\n",
    "            \"offsets\": np.array(encoding[\"offset_mapping\"]),\n",
    "            \"text\": text,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44f60ed7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:04:59.906070Z",
     "iopub.status.busy": "2022-03-25T14:04:59.905076Z",
     "iopub.status.idle": "2022-03-25T14:05:09.083890Z",
     "shell.execute_reply": "2022-03-25T14:05:09.084460Z",
     "shell.execute_reply.started": "2022-03-25T14:01:01.226334Z"
    },
    "papermill": {
     "duration": 9.202498,
     "end_time": "2022-03-25T14:05:09.084647",
     "exception": false,
     "start_time": "2022-03-25T14:04:59.882149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "def plot_annotation(df, pn_num, target):\n",
    "    options = {\"colors\": {}}\n",
    "\n",
    "    df_text = df[df[\"pn_num\"] == pn_num].reset_index(drop=True)\n",
    "\n",
    "    text = df_text[\"pn_history\"][0]\n",
    "    ents = []\n",
    "\n",
    "    for spans, feature_text, feature_num in df_text[[\"span\", \"feature_text\", \"feature_num\"]].values:\n",
    "        if target != feature_text: continue\n",
    "        for s in spans:\n",
    "            ents.append({\"start\": int(s[0]), \"end\": int(s[1]), \"label\": feature_text})\n",
    "\n",
    "        options[\"colors\"][feature_text] =  f\"rgb{tuple(np.random.randint(100, 255, size=3))}\"\n",
    "\n",
    "    doc = {\"text\": text, \"ents\": sorted(ents, key=lambda i: i[\"start\"])}\n",
    "\n",
    "    spacy.displacy.render(doc, style=\"ent\", options=options, manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9faabb29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:09.134138Z",
     "iopub.status.busy": "2022-03-25T14:05:09.133315Z",
     "iopub.status.idle": "2022-03-25T14:05:09.135313Z",
     "shell.execute_reply": "2022-03-25T14:05:09.135721Z",
     "shell.execute_reply.started": "2022-03-25T14:01:05.038193Z"
    },
    "papermill": {
     "duration": 0.034953,
     "end_time": "2022-03-25T14:05:09.135870",
     "exception": false,
     "start_time": "2022-03-25T14:05:09.100917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text, \n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "279cf637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:09.173886Z",
     "iopub.status.busy": "2022-03-25T14:05:09.173255Z",
     "iopub.status.idle": "2022-03-25T14:05:10.442276Z",
     "shell.execute_reply": "2022-03-25T14:05:10.442971Z",
     "shell.execute_reply.started": "2022-03-25T14:01:07.469303Z"
    },
    "papermill": {
     "duration": 1.291091,
     "end_time": "2022-03-25T14:05:10.443173",
     "exception": false,
     "start_time": "2022-03-25T14:05:09.152082",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_workers=4\n",
    "    path=[ \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold0_best.pth\",\n",
    "          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold1_best.pth\",\n",
    "          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold2_best.pth\",\n",
    "          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold3_best.pth\",\n",
    "          \"../input/deberta-deepshare/microsoft-deberta-v3-large_fold4_best.pth\"\n",
    "         ]\n",
    "    config_path='../input/nbme-debertav2-10fold/config.pth'\n",
    "    batch_size=32\n",
    "    fc_dropout=0.2\n",
    "    max_len=354\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    \n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "tokenizer = DebertaV2TokenizerFast.from_pretrained('../input/deberta-tokenizer')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63d4869c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:10.502666Z",
     "iopub.status.busy": "2022-03-25T14:05:10.501454Z",
     "iopub.status.idle": "2022-03-25T14:05:11.138501Z",
     "shell.execute_reply": "2022-03-25T14:05:11.139047Z",
     "shell.execute_reply.started": "2022-03-25T14:01:10.288319Z"
    },
    "papermill": {
     "duration": 0.669209,
     "end_time": "2022-03-25T14:05:11.139249",
     "exception": false,
     "start_time": "2022-03-25T14:05:10.470040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oof7 = pd.read_pickle(\"../input/deberta-deepshare/oof_df01.pkl\")\n",
    "oof8 = pd.read_pickle(\"../input/deberta-deepshare/oof_df23.pkl\")\n",
    "oof9 = pd.read_pickle(\"../input/deberta-deepshare/oof_df4.pkl\")\n",
    "df = pd.concat([oof7, oof8, oof9]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d77fbdd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:11.187376Z",
     "iopub.status.busy": "2022-03-25T14:05:11.180465Z",
     "iopub.status.idle": "2022-03-25T14:05:11.193525Z",
     "shell.execute_reply": "2022-03-25T14:05:11.193965Z",
     "shell.execute_reply.started": "2022-03-25T14:01:13.971172Z"
    },
    "papermill": {
     "duration": 0.037552,
     "end_time": "2022-03-25T14:05:11.194098",
     "exception": false,
     "start_time": "2022-03-25T14:05:11.156546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14300"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = df[[i for i in range(354)]].values\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f1be59c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:11.234102Z",
     "iopub.status.busy": "2022-03-25T14:05:11.233306Z",
     "iopub.status.idle": "2022-03-25T14:05:22.026844Z",
     "shell.execute_reply": "2022-03-25T14:05:22.026320Z",
     "shell.execute_reply.started": "2022-03-25T14:01:17.152964Z"
    },
    "papermill": {
     "duration": 10.815975,
     "end_time": "2022-03-25T14:05:22.026994",
     "exception": false,
     "start_time": "2022-03-25T14:05:11.211019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = preds.reshape((len(preds), 354))\n",
    "char_probs = get_char_probs(df['pn_history'].values, prediction, CFG.tokenizer)\n",
    "results = get_results(char_probs, th=0.47)\n",
    "df['span'] =get_predictions(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a92d8ffc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:22.067923Z",
     "iopub.status.busy": "2022-03-25T14:05:22.062127Z",
     "iopub.status.idle": "2022-03-25T14:05:25.223026Z",
     "shell.execute_reply": "2022-03-25T14:05:25.222245Z",
     "shell.execute_reply.started": "2022-03-25T14:01:28.128704Z"
    },
    "papermill": {
     "duration": 3.17946,
     "end_time": "2022-03-25T14:05:25.223171",
     "exception": false,
     "start_time": "2022-03-25T14:05:22.043711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df['new_location'] = results\n",
    "df['new_location'] = df.apply(lambda x: x['new_location'].split(';'),1)\n",
    "                              \n",
    "for idx in range( len(df) ):\n",
    "    if df['new_location'][idx]==['']:\n",
    "        df['new_location'][idx] = []\n",
    "        \n",
    "df['is_same'] = df.apply(lambda x: x['location']==x['new_location'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2d82e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:25.299934Z",
     "iopub.status.busy": "2022-03-25T14:05:25.289811Z",
     "iopub.status.idle": "2022-03-25T14:05:25.354489Z",
     "shell.execute_reply": "2022-03-25T14:05:25.354062Z",
     "shell.execute_reply.started": "2022-03-25T14:01:31.266159Z"
    },
    "papermill": {
     "duration": 0.114484,
     "end_time": "2022-03-25T14:05:25.354640",
     "exception": false,
     "start_time": "2022-03-25T14:05:25.240156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "check_list = []\n",
    "for i in range( len( df ) ):\n",
    "    if not df['is_same'][i]:\n",
    "        check_list.append( i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02a4d659",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:25.392959Z",
     "iopub.status.busy": "2022-03-25T14:05:25.392325Z",
     "iopub.status.idle": "2022-03-25T14:05:25.394945Z",
     "shell.execute_reply": "2022-03-25T14:05:25.395342Z",
     "shell.execute_reply.started": "2022-03-25T14:01:31.373671Z"
    },
    "papermill": {
     "duration": 0.023869,
     "end_time": "2022-03-25T14:05:25.395493",
     "exception": false,
     "start_time": "2022-03-25T14:05:25.371624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4379"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( check_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a2c611a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:25.437245Z",
     "iopub.status.busy": "2022-03-25T14:05:25.436438Z",
     "iopub.status.idle": "2022-03-25T14:05:25.438383Z",
     "shell.execute_reply": "2022-03-25T14:05:25.438760Z",
     "shell.execute_reply.started": "2022-03-25T14:02:31.700605Z"
    },
    "papermill": {
     "duration": 0.025623,
     "end_time": "2022-03-25T14:05:25.438895",
     "exception": false,
     "start_time": "2022-03-25T14:05:25.413272",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DeepShare 예측값\n",
    "def checking(num):\n",
    "    n=check_list[num]\n",
    "    print(df['pn_history'][n])\n",
    "    print()\n",
    "    print('annotation :  ', df['annotation'][n])\n",
    "    print('feature_text :  ',df['feature_text'][n])\n",
    "    print('location :  ',df['location'][n])\n",
    "    print('new_location :  ',df['new_location'][n])\n",
    "    print()\n",
    "    try:\n",
    "        plot_annotation(df, df['pn_num'][n], df['feature_text'][n])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print('===========================================================================')\n",
    "    print(df['pn_history'][n][413 :427])\n",
    "    print(df['pn_history'][n][419 :427])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "007a0ab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:25.483177Z",
     "iopub.status.busy": "2022-03-25T14:05:25.476810Z",
     "iopub.status.idle": "2022-03-25T14:05:25.489341Z",
     "shell.execute_reply": "2022-03-25T14:05:25.489775Z",
     "shell.execute_reply.started": "2022-03-25T14:03:32.095703Z"
    },
    "papermill": {
     "duration": 0.034067,
     "end_time": "2022-03-25T14:05:25.489913",
     "exception": false,
     "start_time": "2022-03-25T14:05:25.455846",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 yo M w/ no cardiac or arrhythmia PMH presents complaining of 3 months of episodic heart pounding.  Pt reports sensation of heart racing and pounding out of chest about 5-6 times total over the past 3 months.  THis last episode the patient experienced light headedness and had some shortness of breath.  Pt is a college student and has recently began to \"share\" his roommates prescribed aderall approx 2 tabs/week over this time.  Pt does not report any adverse effects of the aderall and reports it has helped him study.  No changes in weight, dizziness, changes in appetite, fevers or chills.  PMH none, PSH none, FHx Father MI, Mom thyroid problem.  Meds none accept aderall (not prescribed) Social: college student in philosophy, 2-3 alcoholic drinks/week, does not smoke, tried marijuana once.  Sexually active w/ 1 female partner uses condoms.  ROS negative accept above.\n",
      "\n",
      "annotation :   ['17 yo']\n",
      "feature_text :   17-year\n",
      "location :   ['0 5']\n",
      "new_location :   ['1 5']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">1\n",
       "<mark class=\"entity\" style=\"background: rgb(136, 251, 110); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    7 yo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">17-year</span>\n",
       "</mark>\n",
       " M w/ no cardiac or arrhythmia PMH presents complaining of 3 months of episodic heart pounding.  Pt reports sensation of heart racing and pounding out of chest about 5-6 times total over the past 3 months.  THis last episode the patient experienced light headedness and had some shortness of breath.  Pt is a college student and has recently began to &quot;share&quot; his roommates prescribed aderall approx 2 tabs/week over this time.  Pt does not report any adverse effects of the aderall and reports it has helped him study.  No changes in weight, dizziness, changes in appetite, fevers or chills.  PMH none, PSH none, FHx Father MI, Mom thyroid problem.  Meds none accept aderall (not prescribed) Social: college student in philosophy, 2-3 alcoholic drinks/week, does not smoke, tried marijuana once.  Sexually active w/ 1 female partner uses condoms.  ROS negative accept above.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===========================================================================\n",
      "ek over this t\n",
      "r this t\n"
     ]
    }
   ],
   "source": [
    "checking(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62b82105",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-25T14:05:25.530452Z",
     "iopub.status.busy": "2022-03-25T14:05:25.528025Z",
     "iopub.status.idle": "2022-03-25T14:05:25.535285Z",
     "shell.execute_reply": "2022-03-25T14:05:25.534867Z"
    },
    "papermill": {
     "duration": 0.027441,
     "end_time": "2022-03-25T14:05:25.535400",
     "exception": false,
     "start_time": "2022-03-25T14:05:25.507959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n17 yo -> 17yo : 3\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "17 yo -> 17yo : 3\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39.410026,
   "end_time": "2022-03-25T14:05:28.738741",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-25T14:04:49.328715",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
